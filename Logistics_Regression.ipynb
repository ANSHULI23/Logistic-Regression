{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Logistics Regression**"
      ],
      "metadata": {
        "id": "q9k2_hcXJtzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Theoretical**"
      ],
      "metadata": {
        "id": "xV2bd8jnJzuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "**1. What is Logistic Regression, and how does it differ from Linear Regression?**\n",
        "\n",
        "* **Linear Regression:** Predicts a continuous numerical output (e.g., house price). It fits a straight line to the data.\n",
        "* **Logistic Regression:** Predicts a categorical output (e.g., yes/no, spam/not spam). It estimates the probability of an event occurring, then classifies based on a threshold. It uses a \"squishing\" function (sigmoid) to constrain the output between 0 and 1.\n",
        "\n",
        "-----\n",
        "\n",
        "**2. What is the mathematical equation of Logistic Regression?**\n",
        "\n",
        "* The equation is: $P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n)}}$\n",
        "    * Where:\n",
        "        * $P(y=1|x)$ is the probability of the output being 1 (e.g., \"yes\").\n",
        "        * $e$ is Euler's number.\n",
        "        * $\\beta_0, \\beta_1, ... \\beta_n$ are the coefficients.\n",
        "        * $x_1, x_2, ... x_n$ are the input features.\n",
        "\n",
        "----\n",
        "\n",
        "**3. Why do we use the Sigmoid function in Logistic Regression?**\n",
        "\n",
        "* The sigmoid function (the fraction in the equation above) \"squishes\" any real number into a value between 0 and 1. This is perfect for probabilities, which must be within that range. It takes the linear output of the input features, and transforms it into a probability.\n",
        "\n",
        "----\n",
        "\n",
        "**4. What is the cost function of Logistic Regression?**\n",
        "\n",
        "* The cost function is called \"cross-entropy\" or \"log loss.\" It measures how wrong our predictions are. It penalizes incorrect predictions more heavily when they are made with high confidence. The goal is to minimize this cost.\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "**5. What is Regularization in Logistic Regression? Why is it needed?**\n",
        "\n",
        "* Regularization adds a penalty to the cost function based on the size of the coefficients. This prevents \"overfitting,\" where the model learns the training data too well and performs poorly on new data. It keeps the model simple.\n",
        "\n",
        "----\n",
        "\n",
        "**6. Explain the difference between Lasso, Ridge, and Elastic Net regression.**\n",
        "\n",
        "* **Ridge (L2):** Adds a penalty proportional to the *square* of the coefficients. Shrinks all coefficients, but rarely makes them exactly zero.\n",
        "* **Lasso (L1):** Adds a penalty proportional to the *absolute value* of the coefficients. Can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
        "* **Elastic Net:** A combination of Ridge and Lasso. It adds both L1 and L2 penalties, providing a balance between feature selection and coefficient shrinkage.\n",
        "\n",
        "----\n",
        "\n",
        "**7. When should we use Elastic Net instead of Lasso or Ridge?**\n",
        "\n",
        "* When you have many features, and you suspect that some are correlated. Elastic Net combines the strengths of both Lasso and Ridge, handling correlated features better than Lasso alone and providing feature selection.\n",
        "\n",
        "----\n",
        "\n",
        "**8. What is the impact of the regularization parameter (Î») in Logistic Regression?**\n",
        "\n",
        "* The regularization parameter (lambda, often represented as \"C\" in libraries, where C=1/lambda) controls the strength of regularization.\n",
        "    * A large lambda (small C) means strong regularization: coefficients shrink more, simplifying the model.\n",
        "    * A small lambda (large C) means weak regularization: the model can become more complex, potentially overfitting.\n",
        "\n",
        "----\n",
        "\n",
        "**9. What are the key assumptions of Logistic Regression?**\n",
        "\n",
        "* Binary output.\n",
        "* Independence of features.\n",
        "* Linearity between features and the log-odds of the outcome.\n",
        "* No severe multicollinearity (high correlation between features).\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "**10. What are some alternatives to Logistic Regression for classification tasks?**\n",
        "\n",
        "* Decision Trees\n",
        "* Random Forests\n",
        "* Support Vector Machines (SVMs)\n",
        "* K-Nearest Neighbors (KNN)\n",
        "* Neural Networks\n",
        "\n",
        "----\n",
        "\n",
        "**11. What are Classification Evaluation Metrics?**\n",
        "\n",
        "* **Accuracy:** Overall correct predictions.\n",
        "* **Precision:** Correct positive predictions out of all positive predictions.\n",
        "* **Recall (Sensitivity):** Correct positive predictions out of all actual positive cases.\n",
        "* **F1-score:** Harmonic mean of precision and recall.\n",
        "* **AUC-ROC:** Area under the Receiver Operating Characteristic curve, measuring the ability to distinguish between classes.\n",
        "\n",
        "\n",
        "-----\n",
        "\n",
        "**12. How does class imbalance affect Logistic Regression?**\n",
        "\n",
        "* If one class is much more frequent than the other, the model may be biased towards the majority class. It might perform well on the majority class but poorly on the minority class. Solutions include oversampling, undersampling, and using class weights.\n",
        "\n",
        "------\n",
        "\n",
        "**13. What is Hyperparameter Tuning in Logistic Regression?**\n",
        "\n",
        "* It's the process of finding the best values for parameters like the regularization strength (C) and the solver. Techniques include grid search and cross-validation.\n",
        "\n",
        "------\n",
        "\n",
        "**14. What are different solvers in Logistic Regression? Which one should be used?**\n",
        "\n",
        "* Solvers are algorithms used to optimize the cost function. Common ones include:\n",
        "    * 'liblinear': Good for small datasets.\n",
        "    * 'lbfgs': Good for larger datasets, supports L2 regularization.\n",
        "    * 'sag' and 'saga': Good for large datasets, handle various regularization types.\n",
        "* The choice depends on the dataset size and the type of regularization.\n",
        "\n",
        "-----\n",
        "\n",
        "**15. How is Logistic Regression extended for multiclass classification?**\n",
        "\n",
        "* **One-vs-Rest (OvR):** Trains a binary classifier for each class against all other classes.\n",
        "* **Softmax Regression:** Generalizes logistic regression to multiple classes by calculating the probability of each class and selecting the class with the highest probability.\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "**16. What are the advantages and disadvantages of Logistic Regression?**\n",
        "\n",
        "* **Advantages:**\n",
        "    * Simple and easy to implement.\n",
        "    * Provides probabilities.\n",
        "    * Efficient for binary classification.\n",
        "    * Easy to interpret coefficients.\n",
        "* **Disadvantages:**\n",
        "    * Assumes linearity.\n",
        "    * Can struggle with complex relationships.\n",
        "    * Sensitive to outliers.\n",
        "\n",
        "\n",
        "----\n",
        "\n",
        "**17. What are some use cases of Logistic Regression?**\n",
        "\n",
        "* Spam detection.\n",
        "* Medical diagnosis (e.g., predicting disease presence).\n",
        "* Credit risk assessment.\n",
        "* Customer churn prediction.\n",
        "\n",
        "------\n",
        "\n",
        "**18. What is the difference between Softmax Regression and Logistic Regression?**\n",
        "\n",
        "* **Logistic Regression:** Binary classification (two classes).\n",
        "* **Softmax Regression:** Multiclass classification (more than two classes). Softmax outputs probabilities for each class, summing to 1.\n",
        "\n",
        "----\n",
        "\n",
        "**19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**\n",
        "\n",
        "* Softmax is generally preferred when the classes are mutually exclusive (an item belongs to only one class).\n",
        "* OvR can be useful when classes are not mutually exclusive (an item can belong to multiple classes).\n",
        "* In many cases, the results are very similar.\n",
        "\n",
        "-----\n",
        "\n",
        "\n",
        "**20. How do we interpret coefficients in Logistic Regression?**\n",
        "\n",
        "* The coefficients represent the change in the log-odds of the outcome for a one-unit change in the predictor, holding other predictors constant.\n",
        "* To make it more interpretable, you can exponentiate the coefficients, which gives you the odds ratio. An odds ratio greater than 1 means the odds of the outcome increase; less than 1 means they decrease.\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "ZmwnSaZNJ3Vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Practical**"
      ],
      "metadata": {
        "id": "nyfpTo4eLM8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset (replace with your data loading)\n",
        "data = pd.DataFrame({\n",
        "    'feature1': [1, 2, 3, 4, 5, 6],\n",
        "    'feature2': [5, 4, 3, 2, 1, 0],\n",
        "    'target_column': [0, 0, 0, 1, 1, 1]  # Binary target\n",
        "})\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data[['feature1', 'feature2']]  # Use a list of feature column names\n",
        "y = data['target_column']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgtCpJjlLPqi",
        "outputId": "e087485d-0bfc-403e-c763-526809f60293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression (penalty='l1') and print the model accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset (replace with your data loading)\n",
        "data = pd.DataFrame({\n",
        "    'feature1': [1, 2, 3, 4, 5, 6],\n",
        "    'feature2': [5, 4, 3, 2, 1, 0],\n",
        "    'target_column': [0, 0, 0, 1, 1, 1]  # Binary target\n",
        "})\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data[['feature1', 'feature2']]\n",
        "y = data['target_column']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')  # 'liblinear' for L1\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Model Accuracy (L1 Regularization):\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeNaxKptLQmi",
        "outputId": "7831c932-356e-48ff-b4a1-fb0528df8897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (L1 Regularization): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression (penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset (replace with your data loading)\n",
        "data = pd.DataFrame({\n",
        "    'feature1': [1, 2, 3, 4, 5, 6],\n",
        "    'feature2': [5, 4, 3, 2, 1, 0],\n",
        "    'target_column': [0, 0, 0, 1, 1, 1]  # Binary target\n",
        "})\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data[['feature1', 'feature2']]\n",
        "y = data['target_column']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2')  # 'l2' is the default\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Model Accuracy (L2 Regularization):\", accuracy)\n",
        "\n",
        "# Print coefficients\n",
        "print(\"Model Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBnBV7AwLQuO",
        "outputId": "c728e317-267b-4ffa-ddf9-0a60d170bdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (L2 Regularization): 1.0\n",
            "Model Coefficients: [[ 0.60372287 -0.60348415]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset (replace with your data loading)\n",
        "data = pd.DataFrame({\n",
        "    'feature1': [1, 2, 3, 4, 5, 6],\n",
        "    'feature2': [5, 4, 3, 2, 1, 0],\n",
        "    'target_column': [0, 0, 0, 1, 1, 1]  # Binary target\n",
        "})\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data[['feature1', 'feature2']]\n",
        "y = data['target_column']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with Elastic Net regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Model Accuracy (Elastic Net Regularization):\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPbAQcJ3LVGX",
        "outputId": "86ec465f-b574-4136-c689-f77f720a8e34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (Elastic Net Regularization): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset (replace with your data loading)\n",
        "data = pd.DataFrame({\n",
        "    'feature1': [1, 2, 3, 4, 5, 6],\n",
        "    'feature2': [5, 4, 3, 2, 1, 0],\n",
        "    'target_column': [0, 0, 1, 1, 2, 2]  # Multiclass target\n",
        "})\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data[['feature1', 'feature2']]\n",
        "y = data['target_column']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model for multiclass classification (One-vs-Rest)\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(\"Model Accuracy (Multiclass - OvR):\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMZhyoPKNIb9",
        "outputId": "a116730b-af41-4a45-bc77-77df50c0957f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (Multiclass - OvR): 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset (replace with your data loading)\n",
        "data = pd.DataFrame({\n",
        "    'feature1': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "    'feature2': [5, 4, 3, 2, 1, 0, 1, 2],\n",
        "    'target_column': [0, 0, 0, 1, 1, 1, 0, 1]  # Binary target\n",
        "})\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = data[['feature1', 'feature2']]\n",
        "y = data['target_column']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid with explicit solver-penalty pairings\n",
        "param_grid = [\n",
        "    {C : [0.1, 1, 10], 'penalty' ['l1'], 'solver'['liblinear']},\n",
        "    {C : [0.1, 1, 10], 'penalty' ['l2'], 'solver' ['liblinear', 'lbfgs', 'sag', 'newton-cg']},\n",
        "    {C : [0.1, 1, 10], 'penalty' ['elasticnet'], 'solver'['saga'], 'l1_ratio'},\n",
        "    {C : [0.1, 1, 10], 'penalty' [None], 'solver' ['lbfgs']}\n",
        "]\n",
        "\n",
        "# Create Logistic Regression model\n",
        "model = LogisticRegression()  # No solver specified here\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)  # cv is for cross-validation\n",
        "\n",
        "# Fit the GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Get the best accuracy\n",
        "best_accuracy = grid_search.best_score_  # Accuracy from cross-validation\n",
        "print(\"Best Accuracy:\", best_accuracy)\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Set Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "RNS8VrfANHV7",
        "outputId": "dc6837d4-753d-40e9-d583-f4cf73eda323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "':' expected after dictionary key (<ipython-input-16-db51a481e8af>, line 24)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-db51a481e8af>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    {C : [0.1, 1, 10], 'penalty' ['l1'], 'solver'['liblinear']},\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m ':' expected after dictionary key\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "ue9n7Ed-NHrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "iOdm23NVNHgr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}